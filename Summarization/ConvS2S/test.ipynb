{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 80, 10]\n[6, 82, 10]\n[6, 80, 10]\na [6, 80, 10]\n[6, 80, 10]\n[6, 82, 10]\n[6, 80, 10]\na [6, 80, 10]\n[6, 80, 10]\n[6, 82, 10]\n[6, 80, 10]\na [6, 80, 10]\n[6, 80, 10]\n[6, 82, 10]\n[6, 80, 10]\na [6, 80, 10]\n[6, 80, 10]\n[6, 82, 10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 80, 10]\na [6, 80, 10]\n[6, 80, 10]\n[6, 82, 10]\n[6, 80, 10]\na [6, 80, 10]\n[6, 80, 10]\n"
     ]
    }
   ],
   "source": [
    "emb_encoder=a1\n",
    "print(emb_encoder.shape.as_list())\n",
    "\n",
    "\n",
    "encoder_outputs=[emb_encoder]\n",
    "PAD_emb=tf.nn.embedding_lookup(emb,[1])\n",
    "\n",
    "for enc_layer in range(6):\n",
    "    with tf.variable_scope(\"encoder_%d\"%enc_layer):\n",
    "        emb_encoder=tf.pad(emb_encoder,paddings=[[0,0],[1,1],[0,0]],constant_values=PAD_emb)\n",
    "        print(emb_encoder.shape.as_list())\n",
    "        encoder_shape=emb_encoder.shape.as_list()\n",
    "\n",
    "        emb_encoder=tf.reshape(emb_encoder,shape=[encoder_shape[0],encoder_shape[1],encoder_shape[2],1])# batch_size,seq_len,emb_dim,1\n",
    "\n",
    "        filtersize=[3,10,1,20]\n",
    "        filter=tf.Variable(initial_value=tf.truncated_normal(filtersize,stddev=0.01),name=\"filter\")\n",
    "        \n",
    "        emb_encoder=tf.nn.conv2d(emb_encoder,filter=filter,strides=[1,1,1,1],padding=\"VALID\",dilations=[1,1,1,1])\n",
    "        emb_encoder=tf.reshape(emb_encoder,shape=[6,-1,10*2])\n",
    "        A,B=tf.split(emb_encoder,2,axis=2)\n",
    "        attn=tf.multiply(A,tf.nn.softmax(B))\n",
    "\n",
    "        print(attn.shape.as_list())\n",
    "\n",
    "        emb_encoder=attn+encoder_outputs[-1]\n",
    "        a=tf.nn.softmax(emb_encoder,axis=1)\n",
    "        print(\"a\",a.shape.as_list())\n",
    "        encoder_outputs.append(emb_encoder)\n",
    "        print(emb_encoder.shape.as_list())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   2.   3.   4.   5.   6.   7.]\n [  0.   0.   0.   0.   0.   0.   0.   0.]\n [ 16.  17.  18.  19.  20.  21.  22.  23.]\n [ 24.  25.  26.  27.  28.  29.  30.  31.]\n [  0.   0.   0.   0.   0.   0.   0.   0.]\n [  0.   0.   0.   0.   0.   0.   0.   0.]]\n[[ 0.57735026  0.          0.57735026  0.57735026  0.          0.        ]\n [ 0.57735026  0.          0.57735026  0.57735026  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array(range(48))\n",
    "a=a.reshape([6,8])\n",
    "a=tf.constant(a,dtype=tf.float32)\n",
    "b=tf.constant([1,0,1,1,0,0],dtype=tf.float32,shape=[6,1])\n",
    "g=tf.constant([[1,0,1,1,0,0],[1,0,1,1,0,0]],dtype=tf.float32,shape=[2,6])\n",
    "\n",
    "c=tf.multiply(b,a)\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "d,f=sess.run([c,tf.nn.l2_normalize(g,1)])\n",
    "print(d)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 10]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "a = tf.constant(1)\n",
    "n = tf.constant(10)\n",
    "\n",
    "def cond(a, n):\n",
    "    return  a< n\n",
    "def body(a,n):\n",
    "    a = a + 1\n",
    "    return a,n\n",
    "\n",
    "a, n = tf.while_loop(cond, body, [a, n])\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    res = sess.run([a,n])\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[615, 170],\n       [ 84, 315],\n       [168,  69],\n       [ 23,  45],\n       [390,  78]], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a=tf.constant([[123,34],[12,45],[56,23],[23,45],[65,13]],dtype=tf.int32)\n",
    "b=tf.constant([[5],[7],[3],[1],[6]],tf.int32)\n",
    "tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "v=sess.run([a*b])\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "class A():\n",
    "    def __init__(self,a,b):\n",
    "        self.a=a\n",
    "        self.b=b\n",
    "    \n",
    "    def __call__(self, rouge):\n",
    "        if \"a\"==rouge:\n",
    "            return self.a+self.b\n",
    "        elif \"b\" ==rouge:\n",
    "            return self.a*self.b\n",
    "\n",
    "a=A(3,2)\n",
    "print(a(\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_17:0\", shape=(3, 4), dtype=int32)\nTensor(\"Cast_7:0\", shape=(1,), dtype=float32)\n[5. 5.]\n"
     ]
    }
   ],
   "source": [
    "a=tf.Variable([])\n",
    "table = tf.TensorArray(tf.int32, 12, clear_after_read=False, element_shape=[])\n",
    "table=table.write(1,5)\n",
    "table = tf.reshape(table.stack(), [3, 4])\n",
    "print(table)\n",
    "value=table[0,1]\n",
    "value=tf.reshape(value,shape=[1])\n",
    "value=tf.cast(value,tf.float32)\n",
    "print(value)\n",
    "a=tf.concat([a,value],0)\n",
    "a=tf.concat([a,value],0)\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词汇表中对应的主题词\n",
    "a=tf.constant([[11,1],[12,2],[13,3],[14,4],[15,5],[16,6]],dtype=tf.int32)\n",
    "# 词汇表\n",
    "# b=tf.constant([[1,11,13],[3,16,6],[5,6,2]],dtype=tf.int32)\n",
    "b=tf.constant([[0,13],[1,16],[0,8]],dtype=tf.int32)\n",
    "\n",
    "emb=tf.get_variable(name='embedding',shape=[20,5],dtype=tf.float32)\n",
    "topic_emb=tf.get_variable(name='topics',shape=[10,5],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"while_1/concat:0\", shape=(?, 1, 5), dtype=float32)\nTensor(\"Reshape_5:0\", shape=(?, 2, 5), dtype=float32)\n(3, 2, 5)\n[[[ 0.13084796 -0.3908043   0.19348922 -0.24810493  0.44234452]\n  [ 0.05184722 -0.04054689  0.41993362  0.5034165   0.25516206]]\n\n [[-0.15891024 -0.00476465 -0.01022813  0.37674114 -0.00997618]\n  [ 0.2298823  -0.06199652 -0.4847191   0.60867935 -0.6107369 ]]\n\n [[ 0.13084796 -0.3908043   0.19348922 -0.24810493  0.44234452]\n  [-0.04860163  0.48754075 -0.3807378  -0.12713578  0.21223983]]]\n------\n[[ 0.21192467  0.04543304  0.36380446  0.31198138 -0.59245765]\n [ 0.50931627  0.48423976  0.6293971  -0.51611584 -0.31456184]\n [-0.17607754 -0.3145715   0.45998472 -0.23318592 -0.01605123]\n [ 0.05184722 -0.04054689  0.41993362  0.5034165   0.25516206]\n [ 0.04833382  0.3463604   0.4591692   0.29215127  0.6235923 ]\n [ 0.4013316   0.16012675  0.03698754 -0.37010384 -0.35769206]\n [ 0.2298823  -0.06199652 -0.4847191   0.60867935 -0.6107369 ]\n [-0.05478263  0.448682    0.13338143  0.37152642  0.01579231]\n [-0.41179025  0.08770227  0.18963516 -0.14332855 -0.15299988]\n [-0.09382129  0.09068942 -0.0051555   0.30831826 -0.5896685 ]]\n------\n[[ 0.13084796 -0.3908043   0.19348922 -0.24810493  0.44234452]\n [-0.15891024 -0.00476465 -0.01022813  0.37674114 -0.00997618]\n [-0.00377032  0.26119843 -0.23634824  0.1519849   0.4030935 ]\n [-0.37604547  0.3715562   0.02086845  0.11135718  0.05472419]\n [ 0.16493961  0.4489899   0.11456558  0.01513752  0.3019223 ]\n [-0.42482787 -0.46412095 -0.09880072  0.07635698  0.16157892]\n [ 0.18086693  0.45991096 -0.3466398   0.48279586 -0.42687786]\n [ 0.2698711   0.2765884   0.16423604 -0.4384499  -0.03918397]\n [-0.04860163  0.48754075 -0.3807378  -0.12713578  0.21223983]\n [ 0.36251083 -0.46090692 -0.1707257  -0.17288104  0.4034051 ]\n [ 0.4193003   0.19245848 -0.11220068  0.24648932 -0.17894158]\n [-0.35475034 -0.37168646 -0.06579047  0.35050717  0.13765845]\n [-0.14070487  0.350006    0.47299674  0.11930206 -0.01222974]\n [-0.01491803 -0.01869783  0.3582178  -0.34374326 -0.12258825]\n [ 0.21423307  0.4493899  -0.33341736  0.29823127  0.30026194]\n [ 0.35148713  0.45641723  0.10066512 -0.45752227 -0.03225675]\n [ 0.1828048   0.05804297  0.08280566  0.18865123 -0.47838804]\n [-0.35383612  0.08558002 -0.02767208 -0.3715589   0.40112188]\n [ 0.31943783  0.0417147  -0.04916379 -0.05038539 -0.2998146 ]\n [ 0.01808986  0.04191616  0.32623842  0.44058523 -0.26368475]]\n"
     ]
    }
   ],
   "source": [
    "a_shape=a.get_shape()[0]\n",
    "b_shape=b.get_shape()\n",
    "size=b_shape[0]*b_shape[1]\n",
    "\n",
    "def look_step(g,index,single,k1):\n",
    "    t=tf.cond(tf.equal(single,a[k1,0]),\n",
    "            lambda :(True,k1),\n",
    "            lambda :(False,-1))\n",
    "    return tf.logical_or(g,t[0]),tf.maximum(index,t[1]),single,k1+1\n",
    "    \n",
    "def _look(single):\n",
    "    g=False\n",
    "    k1=0\n",
    "    index=-1\n",
    "    g,index,*_=tf.while_loop(\n",
    "        cond=lambda g,index,single,k1:k1<a_shape,\n",
    "        body=look_step,\n",
    "        loop_vars=[g,index,single,k1]\n",
    "    )\n",
    "    return g,index\n",
    "\n",
    "k=tf.constant(1)\n",
    "def loop_step(k,topic,matrix):\n",
    "    g,index=_look(topic[k])\n",
    "    matr=tf.cond(g,\n",
    "                lambda :tf.gather(topic_emb,[a[index,1]]),\n",
    "                lambda :tf.gather(emb,[topic[k]]))\n",
    "    matr=tf.reshape(matr,[1,1,-1])\n",
    "    matrix=tf.concat([matrix,matr],0)\n",
    "    print(matrix)\n",
    "    return k+1,topic,matrix\n",
    "\n",
    "g=tf.reshape(b,[-1])\n",
    "# matrix=tf.Variable([[]])\n",
    "matrix=tf.nn.embedding_lookup(emb,[0])\n",
    "matrix=tf.reshape(matrix,[1,1,-1])\n",
    "_,_,matrix=tf.while_loop(\n",
    "    cond=lambda k,b,*_:k<size,\n",
    "    body=loop_step,\n",
    "    loop_vars=[k,g,matrix],\n",
    "    shape_invariants=[k.get_shape(),tf.TensorShape([None]),tf.TensorShape([None,1,5])]\n",
    "\n",
    ")\n",
    "\n",
    "matrix=tf.reshape(matrix,shape=[-1,b_shape[1],5])\n",
    "print(matrix)\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "matrix,topics,emm=sess.run([matrix,topic_emb,emb])\n",
    "print(np.array(matrix).shape)\n",
    "print(matrix)\n",
    "\n",
    "print('------')\n",
    "print(topics)\n",
    "print('------')\n",
    "print(emm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12611371  0.08051919  2.6596558  -2.5261319   1.4858145 ]\n[-0.12611371 -1.0288202   0.08051919 -0.40016657  2.6596558   0.4868604\n -2.5261319   0.5280155   1.4858145  -0.3094576 ]\n"
     ]
    }
   ],
   "source": [
    "i = tf.constant(0)\n",
    "l = tf.Variable([])\n",
    "array = tf.Variable(tf.random_normal([10]))\n",
    "def body(i, l):                                               \n",
    "    temp = tf.gather(array,i)\n",
    "    l = tf.concat([l, [temp]], 0)\n",
    "    return i+2, l\n",
    "def cond(i,l):\n",
    "   return i < 10\n",
    "index, list_vals = tf.while_loop(cond, body, [i, l],shape_invariants=[i.get_shape(),\n",
    "                                                   tf.TensorShape([None])])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "a1,b1=sess.run([list_vals,array])\n",
    "print(a1)\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat_18:0\", shape=(2, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gh1=tf.gather(emb,[1])\n",
    "gh2=tf.gather(emb,[0])\n",
    "gh=tf.concat([gh2,gh1],axis=0)\n",
    "print(gh)\n",
    "a1,b1=sess.run([emb,gh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_17:0\", shape=(4, 3, 3), dtype=int32)\nTensor(\"ExpandDims:0\", shape=(4, 1, 3), dtype=int32)\n[[[ 7  7  7]]\n\n [[ 8  8  8]]\n\n [[ 9 13  9]]\n\n [[ 9  9  9]]]\n[[[ 7  7  7]]\n\n [[ 8  8  8]]\n\n [[ 9 13  9]]\n\n [[ 9  9  9]]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[[1, 1, 1], [2, 2, 2], [7, 7, 7]],\n",
    "                 [[3, 3, 3], [4, 4, 4], [8, 8, 8]],\n",
    "                 [[5, 5, 5], [6, 12, 6], [9, 13, 9]],\n",
    "                 [[5,12, 5], [6, 6, 6], [9, 9, 9]]])\n",
    "\n",
    "# z1 = tf.strided_slice(t, [1], [-1], [1])\n",
    "z2 = tf.strided_slice(t, [0, 2], [4, 3], [1, 1])\n",
    "z3 = tf.strided_slice(t, [1, 0, 1], [-1, 2, 3], [1, 1, 1])\n",
    "z4=tf.expand_dims(z2[:,-1,:],1)\n",
    "print(t)\n",
    "print(z4)\n",
    "with tf.Session() as sess:\n",
    "    # print(sess.run(z1))\n",
    "    print(sess.run(z2))\n",
    "    print(sess.run(z4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Max_1:0\", shape=(3, 4), dtype=float32)\n[array([[ 4.,  9., 14., 19.],\n       [24., 29., 34., 39.],\n       [44., 49., 54., 59.]], dtype=float32), array([[4, 4, 4, 4],\n       [4, 4, 4, 4],\n       [4, 4, 4, 4]])]\n"
     ]
    }
   ],
   "source": [
    "s=np.array(range(60))\n",
    "\n",
    "a=tf.constant(s,dtype=tf.float32,shape=[3,4,5])\n",
    "c=tf.reduce_max(a,-1)\n",
    "print(c)\n",
    "d=tf.argmax(a,-1)\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "print(sess.run([c,d]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(3, 1), dtype=float32)\nTensor(\"Reshape:0\", shape=(?, 1), dtype=float32)\n[array([[0.9740323 ],\n       [2.1492686 ],\n       [0.22505063],\n       [0.9740323 ],\n       [2.1492686 ],\n       [0.22505063],\n       [0.9740323 ],\n       [2.1492686 ],\n       [0.22505063],\n       [0.9740323 ],\n       [2.1492686 ],\n       [0.22505063],\n       [0.9740323 ],\n       [2.1492686 ],\n       [0.22505063]], dtype=float32), array([[-0.02596773],\n       [ 1.1492685 ],\n       [-0.7749494 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "i0 = tf.constant(0)\n",
    "m0 = tf.ones([3, 1])\n",
    "# x = tf.get_variable('www', shape=(3,1), initializer=tf.zeros_initializer)\n",
    "# loop = 5\n",
    "# def _cond(i0, m0):\n",
    "#   return tf.less(i0, loop-1)\n",
    "# \n",
    "# def _res(i0, m0):\n",
    "#   n = tf.ones([3, 1]) + x\n",
    "#   m0 = tf.concat([m0, n], axis=0)\n",
    "#   return i0+1, m0\n",
    "\n",
    "# loop_vars=[tf.constant(0,tf.int32),tf.TensorArray(m0.dtype,size=loop)]\n",
    "# i0, m0 = tf.while_loop(\n",
    "#     _cond, _res, loop_vars=[i0, m0],\n",
    "#     shape_invariants=[i0.get_shape(), tf.TensorShape([None, 1])])\n",
    "# \n",
    "# m0.set_shape([loop*3,1])\n",
    "# opt = tf.train.AdagradOptimizer(1)\n",
    "# \n",
    "# grad = opt.compute_gradients(m0)\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# \n",
    "# print(sess.run(grad))\n",
    "\n",
    "def mystack(x, n):\n",
    "    loop_vars = [\n",
    "        tf.constant(0, tf.int32),\n",
    "        tf.TensorArray(x.dtype, size=n),\n",
    "    ]\n",
    "    _, fx = tf.while_loop(\n",
    "        lambda j, _: j < n,\n",
    "        lambda j, result: (j + 1, result.write(j, 1+x)),\n",
    "        loop_vars= loop_vars\n",
    "    )\n",
    "    return tf.reshape(fx.stack(), (-1,1) )\n",
    " \n",
    "x = tf.constant(np.random.randn(3,1), tf.float32 )\n",
    "print(x)\n",
    "loop = 5\n",
    "m = mystack(x,loop)\n",
    "print(m)\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([m,x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# 22 scope (name_scope/variable_scope)\n",
    "from __future__ import print_function\n",
    " \n",
    "class TrainConfig:\n",
    "    batch_size = 20\n",
    "    time_steps = 20\n",
    "    input_size = 10\n",
    "    output_size = 2\n",
    "    cell_size = 11\n",
    "    learning_rate = 0.01\n",
    " \n",
    " \n",
    "class TestConfig(TrainConfig):\n",
    "    time_steps = 1\n",
    " \n",
    " \n",
    "class RNN(object):\n",
    " \n",
    "    def __init__(self, config):\n",
    "        self._batch_size = config.batch_size\n",
    "        self._time_steps = config.time_steps\n",
    "        self._input_size = config.input_size\n",
    "        self._output_size = config.output_size\n",
    "        self._cell_size = config.cell_size\n",
    "        self._lr = config.learning_rate\n",
    "        self._built_RNN()\n",
    " \n",
    "    def _built_RNN(self):\n",
    "        with tf.variable_scope('inputs'):\n",
    "            self._xs = tf.placeholder(tf.float32, [self._batch_size, self._time_steps, self._input_size], name='xs')\n",
    "            self._ys = tf.placeholder(tf.float32, [self._batch_size, self._time_steps, self._output_size], name='ys')\n",
    "        with tf.name_scope('RNN'):\n",
    "            with tf.variable_scope('input_layer'):\n",
    "                l_in_x = tf.reshape(self._xs, [-1, self._input_size], name='2_2D')  # (batch*n_step, in_size)\n",
    "                # Ws (in_size, cell_size)\n",
    "                Wi = self._weight_variable([self._input_size, self._cell_size])\n",
    "                print(Wi.name)\n",
    "                # bs (cell_size, )\n",
    "                bi = self._bias_variable([self._cell_size, ])\n",
    "                # l_in_y = (batch * n_steps, cell_size)\n",
    "                with tf.name_scope('Wx_plus_b'):\n",
    "                    l_in_y = tf.matmul(l_in_x, Wi) + bi\n",
    "                l_in_y = tf.reshape(l_in_y, [-1, self._time_steps, self._cell_size], name='2_3D')\n",
    " \n",
    "            with tf.variable_scope('cell'):\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(self._cell_size)\n",
    "                with tf.name_scope('initial_state'):\n",
    "                    self._cell_initial_state = cell.zero_state(self._batch_size, dtype=tf.float32)\n",
    " \n",
    "                self.cell_outputs = []\n",
    "                cell_state = self._cell_initial_state\n",
    "                for t in range(self._time_steps):\n",
    "                    if t > 0: tf.get_variable_scope().reuse_variables()\n",
    "                    cell_output, cell_state = cell(l_in_y[:, t, :], cell_state)\n",
    "                    self.cell_outputs.append(cell_output)\n",
    "                self._cell_final_state = cell_state\n",
    " \n",
    "            with tf.variable_scope('output_layer'):\n",
    "                # cell_outputs_reshaped (BATCH*TIME_STEP, CELL_SIZE)\n",
    "                cell_outputs_reshaped = tf.reshape(tf.concat(self.cell_outputs, 1), [-1, self._cell_size])\n",
    "                Wo = self._weight_variable((self._cell_size, self._output_size))\n",
    "                bo = self._bias_variable((self._output_size,))\n",
    "                product = tf.matmul(cell_outputs_reshaped, Wo) + bo\n",
    "                # _pred shape (batch*time_step, output_size)\n",
    "                self._pred = tf.nn.relu(product)    # for displacement\n",
    " \n",
    "        with tf.name_scope('cost'):\n",
    "            _pred = tf.reshape(self._pred, [self._batch_size, self._time_steps, self._output_size])\n",
    "            mse = self.ms_error(_pred, self._ys)\n",
    "            mse_ave_across_batch = tf.reduce_mean(mse, 0)\n",
    "            mse_sum_across_time = tf.reduce_sum(mse_ave_across_batch, 0)\n",
    "            self._cost = mse_sum_across_time\n",
    "            self._cost_ave_time = self._cost / self._time_steps\n",
    " \n",
    "        with tf.variable_scope('trian'):\n",
    "            self._lr = tf.convert_to_tensor(self._lr)\n",
    "            self.train_op = tf.train.AdamOptimizer(self._lr).minimize(self._cost)\n",
    " \n",
    "    @staticmethod\n",
    "    def ms_error(y_target, y_pre):\n",
    "        return tf.square(tf.subtract(y_target, y_pre))\n",
    " \n",
    "    @staticmethod\n",
    "    def _weight_variable(shape, name='weights'):\n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=0.5, )\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    " \n",
    "    @staticmethod\n",
    "    def _bias_variable(shape, name='biases'):\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn12/input_layer/weights:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn12/input_layer/weights:0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_config = TrainConfig()  #定义train_config\n",
    "    test_config = TestConfig()\n",
    " \n",
    "#     # the wrong method to reuse parameters in train rnn\n",
    "#     with tf.variable_scope('train_rnn'):\n",
    "#         train_rnn1 = RNN(train_config)\n",
    "#     with tf.variable_scope('test_rnn'):\n",
    "#         test_rnn1 = RNN(test_config)\n",
    " \n",
    "    # the right method to reuse parameters in train rnn\n",
    "    #目的使train的RNN调用参数，然后利用variable_scope方法共享RNN,让test的RNN再次调用一样的参数，\n",
    "    with tf.variable_scope('rnn12') as scope:\n",
    "        \n",
    "        train_rnn2 = RNN(train_config)\n",
    "        scope.reuse_variables()        #告诉TF想重复利用RNN的参数\n",
    "        test_rnn2 = RNN(test_config)\n",
    "        sess = tf.Session()\n",
    "        # tf.initialize_all_variables() no long valid from\n",
    "        # 2017-03-02 if using tensorflow >= 0.12\n",
    "        if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "            init = tf.initialize_all_variables()\n",
    "        else:\n",
    "            init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
