{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "files='./NLP/NER/data/BosonNLP_NER_6C.txt'\n",
    "a={'I-TIME', 'I-PER', 'B-CRIME', 'I-CRIME', 'B-TIME', 'B-LOC', 'B-ORG', \n",
    "   'B-ROLE', 'B-PER', 'I-ROLE', 'I-ORG', 'I-LOC', 'I-LAW', 'B-LAW', 'O'}\n",
    "import jieba,re\n",
    "f=open(files,'r')\n",
    "lines=f.readlines()\n",
    "compiles=re.compile(\"{{(.*?)}}\")\n",
    "filter_compile=re.compile(\"[a-zA-Z_]+:\")\n",
    "compile_url=re.compile(\"(http[s]?:[a-zA-Z0-9\\.\\/\\?#&\\+=]+)\")\n",
    "tags=[]\n",
    "words=[]\n",
    "for line in lines:\n",
    "    split_line=compiles.split(line)\n",
    "    tag=[]\n",
    "    word=[]\n",
    "    for lin in split_line:\n",
    "        if len(lin.strip())==0:\n",
    "            continue\n",
    "        lin=compile_url.sub(\"URL\",lin)\n",
    "        if filter_compile.match(lin):\n",
    "            entity_name,entity=lin.split(\":\",1)\n",
    "            entity=list(jieba.cut(entity))\n",
    "            entities=[a.strip() for a in entity if len(a.strip())!=0]\n",
    "            word.extend(entities)\n",
    "            if entity_name=='person_name':\n",
    "                tag.append(\"B-PER\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-PER\"]*(len(entities)-1))\n",
    "            elif entity_name=='time':\n",
    "                tag.append(\"B-TIME\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-TIME\"]*(len(entities)-1))\n",
    "            elif entity_name=='location':\n",
    "                tag.append(\"B-LOC\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-LOC\"]*(len(entities)-1))\n",
    "            elif entity_name=='org_name':\n",
    "                tag.append(\"B-ORG\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-ORG\"]*(len(entities)-1))\n",
    "            elif entity_name=='company_name':\n",
    "                tag.append(\"B-COMP\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-COMP\"]*(len(entities)-1))\n",
    "            elif entity_name=='product_name':\n",
    "                tag.append(\"B-PRO\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-PRO\"]*(len(entities)-1))\n",
    "        else:\n",
    "            entity=list(jieba.cut(lin))\n",
    "            entities=[a.strip() for a in entity if len(a.strip())!=0]\n",
    "            word.extend(entities)\n",
    "            tag.extend([\"O\"]*len(entities))\n",
    "    if len(word)!=len(tag):\n",
    "        raise ValueError(len(word),len(tag),line)\n",
    "    words.append(word)\n",
    "    tags.append(tag)\n",
    "print(len(words),len(tags))\n",
    "files1='./NLP/NER/data/target.txt'\n",
    "files2='./NLP/NER/data/source.txt'\n",
    "f1=open(files1,'w')\n",
    "for tag in tags:\n",
    "    f1.write(\" \".join(tag)+\"\\n\")\n",
    "f1.close()\n",
    "f2=open(files2,'w')\n",
    "for word in words:\n",
    "    f2.write(\" \".join(word)+\"\\n\")\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['韩国', 'doctorcos', '氨基酸', '核糖', '睡眠', '面膜', '，', '一盒', '足以', '满足', '改善', '细纹', '，', '美', '白', '，', '收', '细', '毛孔', '，', '修护', '肌肤', '，', '改善', '敏感', '，', '补水', '保湿', '，', '紧', '致', '肌肤', '！', '涂', '到', '脸上', '抹', '匀', '后', '按摩', '就', '会', '有', '一颗颗', '的', '小', '水珠', '出来', '！', '所以', '称作', '爆', '水', '神器', '，', '补水', '效果', '很', '明显', '，', '跟', '面霜', '一样', '涂上一层', '按摩', '一下', '就', '睡觉', '，', '第二天', '脸', '摸', '起来', '超级', '嫩', '滑', '饱满', '，', '也', '非常', '好', '上妆', '！']\n['J-PER', 'B-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER', 'Z-PER']\n"
     ]
    }
   ],
   "source": [
    "line='{{location:韩国}} {{product_name:doctorcos氨基酸核糖睡眠面膜}}，一盒足以满足改善细纹，美白，收细毛孔，修护肌肤，改善敏感，补水保湿，紧致肌肤！涂到脸上抹匀后按摩就会有一颗颗的小水珠出来！所以称作爆水神器，补水效果很明显，跟面霜一样涂上一层按摩一下就睡觉，{{time:第二天}}脸摸起来超级嫩滑饱满，也非常好上妆！'\n",
    "\n",
    "entity_tag={\"TIME\":\"time\",\"LOC\":\"location\",\"PER\":\"person_name\",\"ORG\":\"org_name\",\"COMP\":\"company_name\",\"PRO\":\"product_name\"}\n",
    "import jieba,re\n",
    "def drop_tag(seqence,tag):\n",
    "    compiles=re.compile(\"{{(.*?)}}\")\n",
    "    filter_compile=re.compile(\"[a-zA-Z_]+:\")\n",
    "    seqences=compiles.split(seqence)\n",
    "    seqs=[]\n",
    "    for seq in seqences:\n",
    "        if len(seq.strip())==0:\n",
    "            continue\n",
    "        if filter_compile.match(seq):\n",
    "            name,entity=seq.split(\":\",1)\n",
    "            if name==entity_tag[tag]:\n",
    "                seqs.append(\" START \"+entity+\" END \")\n",
    "            else:\n",
    "                seqs.append(entity)\n",
    "        else:\n",
    "            seqs.append(seq)\n",
    "    return \"\".join(seqs)\n",
    "\n",
    "def cut_seq(seqence):\n",
    "    compile_url=re.compile(\"(http[s]?:[a-zA-Z0-9\\.\\/\\?#&\\+=]+)\")\n",
    "    seqence=compile_url.sub(\"URL\",seqence)\n",
    "    cuts=list(jieba.cut(seqence,HMM=False))\n",
    "    cut=[a.strip() for a in cuts if len(a.strip())!=0]\n",
    "    return cut\n",
    "\n",
    "def find_tag(seqs,tag=\"START\"):\n",
    "    if isinstance(tag,list):\n",
    "        index=[ind for ind,word in enumerate(seqs) if word in tag]\n",
    "    else:\n",
    "        index=[ind for ind,word in enumerate(seqs) if word==tag]\n",
    "    return index\n",
    "\n",
    "def locEntity(start,end,seqs,tag):\n",
    "    xrange=end-start-1\n",
    "    piece_tag=seqs[start+1:end]\n",
    "    if xrange==1:\n",
    "        tag.append('J-PER')\n",
    "    elif xrange==2:\n",
    "        if len(piece_tag[0])>=2:\n",
    "            if piece_tag[0] in doubelsurname:\n",
    "                tag.append(\"C-PER\")\n",
    "            else:\n",
    "                tag.append(\"I-PER\")\n",
    "                \n",
    "            if len(piece_tag[1])==2:\n",
    "                tag.append(\"K-PER\")\n",
    "            else:\n",
    "                tag.append(\"E-PER\")\n",
    "        else:\n",
    "            tag.append(\"C-PER\")\n",
    "            if len(piece_tag[1])>=2:\n",
    "                tag.append(\"K-PER\")\n",
    "            else:\n",
    "                tag.append(\"F-PER\")\n",
    "    else:\n",
    "        if len(piece_tag[0])>=2:\n",
    "            if piece_tag[0] in doubelsurname:\n",
    "                tag.append(\"C-PER\")\n",
    "            else:\n",
    "                tag.append(\"I-PER\")\n",
    "        else:\n",
    "            tag.append(\"C-PER\")\n",
    "        \n",
    "        for i in range(1,len(piece_tag)-1):\n",
    "            if len(piece_tag[i])>=2:\n",
    "                tag.append(\"K-PER\")\n",
    "            else:\n",
    "                tag.append(\"D-PER\")\n",
    "        if len(piece_tag[-1])>=2:\n",
    "            tag.append(\"K-PER\")\n",
    "        else:\n",
    "            tag.append(\"E-PER\")\n",
    "            \n",
    "\n",
    "files='./NLP/NER/data/BosonNLP_NER_6C.txt'\n",
    "lines=open(files,'r').readlines()\n",
    "# for line in lines:\n",
    "seqences=drop_tag(line,\"LOC\")\n",
    "seqs=cut_seq(seqences)\n",
    "init_seq=re.sub(\"START|END\",'',seqences)\n",
    "init_seqs=cut_seq(init_seq)\n",
    "print(init_seqs )\n",
    "new_seq=[]\n",
    "tag=[]\n",
    "index=find_tag(seqs,tag=[\"START\",\"END\"])\n",
    "index=sorted(index)\n",
    "reduce=[]\n",
    "for i in range(0,len(index),2):\n",
    "    reduce.append(index[i+1]-index[i])\n",
    "entity_length=[]\n",
    "j=-1\n",
    "judge=True\n",
    "start=0\n",
    "end=0\n",
    "last=0\n",
    "i=0\n",
    "while j<len(seqs)-1:\n",
    "    j+=1\n",
    "    if seqs[j]==\"START\":\n",
    "        if len(tag)>0:\n",
    "            if tag[-1]!=\"X-PER\" and tag[-1]==\"Z-PER\":\n",
    "                tag[-1]=\"A-PER\"\n",
    "        start=j\n",
    "        j=end=reduce[i]+start\n",
    "        locEntity(start,end,seqs,tag)\n",
    "        i+=1\n",
    "        if len(reduce)>i:\n",
    "            if seqs[j+1]==\"START\":\n",
    "                continue\n",
    "            if seqs[j+2]=='START':\n",
    "                tag.append(\"X-PER\")\n",
    "            else:\n",
    "                tag.append('B-PER')\n",
    "            j+=1\n",
    "        else:\n",
    "            if len(seqs)>j+1:\n",
    "                tag.append('B-PER')\n",
    "                j+=1\n",
    "    else:\n",
    "        tag.append(\"Z-PER\")\n",
    "\n",
    "    \n",
    "print(init_seqs)\n",
    "print(tag)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
