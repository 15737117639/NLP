{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line=\"{{product_name:浙江在线杭州}}{{time:4月25日}}讯（记者{{person_name: 施宇翔}} 通讯员 {{person_name:方英}}）毒贩很“时髦”，用{{product_name:微信}}交易毒品。没料想警方也很“潮”，将计就计，一举将其擒获。记者从{{org_name:杭州江干区公安分局}}了解到，经过一个多月的侦查工作，{{org_name:江干区禁毒专案组}}抓获吸贩毒人员5名，缴获“冰毒”400余克，毒资30000余元，扣押汽车一辆。{{location:黑龙江}}籍男子{{person_name:钱某}}长期落脚于宾馆、单身公寓，经常变换住址。他有一辆车，经常半夜驾车来往于{{location:杭州主城区}}的各大宾馆和单身公寓，并且常要活动到{{time:凌晨6、7点钟}}，{{time:白天}}则在家里呼呼大睡。{{person_name:钱某}}不寻常的特征，引起了警方注意。禁毒大队通过侦查，发现{{person_name:钱某}}实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。\"\n",
    "entity=[\"time\",\"location\",\"person_name\",\"org_name\",\"company_name\",\"product_name\"]\n",
    "import re,jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /var/folders/83/ng05nj_s1_s9842rkp7xn5jc0000gp/T/jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.737 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['黑龙江']\n['杭州', '主城区']\n"
     ]
    }
   ],
   "source": [
    "newline=[]\n",
    "loc_entity=[]\n",
    "person_entity=[]\n",
    "org_entity=[]\n",
    "doubelsurname=['欧阳', '太史', '端木', '上官', '司马', '东方', '独孤', '南宫', '万俟', '闻人', '夏侯', '诸葛', '尉迟', '公羊', '赫连', '澹台', '皇甫', '宗政', '濮阳', '公冶', '太叔', '申屠', '公孙', '慕容', '仲孙', '钟离', '长孙', '宇文', '司徒', '鲜于', '司空', '闾丘', '子车', '亓官', '司寇', '巫马', '公西', '颛孙', '壤驷', '公良', '漆雕', '乐正', '宰父', '谷梁', '拓跋', '夹谷', '轩辕', '令狐', '段干', '百里', '呼延', '东郭', '南门', '羊舌', '微生', '公户', '公玉', '公仪', '梁丘', '公仲', '公上', '公门', '公山', '公坚', '左丘', '公伯', '西门', '公祖', '第五', '公乘', '贯丘', '公皙', '南荣', '东里', '东宫', '仲长', '子书', '子桑', '即墨', '达奚', '褚师', '吴铭']\n",
    "files='./NLP/NER/data/xinhua_dict.txt'\n",
    "f=open(files,'r')\n",
    "words=f.readlines()\n",
    "f.close()\n",
    "\n",
    "def append_entity(entity,newline):\n",
    "    entity=list(jieba.cut(entity))\n",
    "    for ent in entity:\n",
    "        newline.append(ent)\n",
    "    return newline,entity\n",
    "\n",
    "def perEntity(entity):\n",
    "    name=[]\n",
    "    length=len(entity)\n",
    "    if length!=1:\n",
    "        if entity[0] not in doubelsurname and len(entity[0])==2:\n",
    "            name.append(\"C\")\n",
    "            \n",
    "context_u=False #上下文的上文\n",
    "context_d=False #上下文的下文\n",
    "compile=re.compile(\"{{(.*?)}}\")\n",
    "filter_compile=re.compile(\"\\w+:\")\n",
    "split_line=compile.split(line)\n",
    "for ind,lin in enumerate(split_line):\n",
    "    if len(lin)==0:\n",
    "        continue\n",
    "    if filter_compile.match(lin):\n",
    "        entity_name,entity=lin.split(\":\")\n",
    "        newline,entity_cut=append_entity(entity,newline)\n",
    "        entity_len=len(entity_cut)  \n",
    "        if entity_name=='person_name':\n",
    "            # 使用jieba分词，如果将人名分成一个单词，那么先判断前两个字是否为复姓，然后后面的按照正常的给予标签\n",
    "            if entity_len==1:\n",
    "                if entity[:2] in doubelsurname:\n",
    "                    person_entity.append(\"C\")\n",
    "                    if len(entity[2:])==1:\n",
    "                        person_entity.append(\"F\")\n",
    "                    else:\n",
    "                        for _ in range(len(entity[2:-1])):\n",
    "                            person_entity.append(\"D\")\n",
    "                        person_entity.append(\"E\")\n",
    "                elif entity[:2] in words and entity_len!=2:\n",
    "                    person_entity.append(\"I\")\n",
    "                    if len(entity[2:])==1:\n",
    "                        person_entity.append(\"F\")\n",
    "                    else:\n",
    "                        for _ in range(len(entity[2:-1])):\n",
    "                            person_entity.append(\"D\")\n",
    "                        person_entity.append(\"E\")\n",
    "                elif entity[:2] in words and entity_len==2:\n",
    "                    person_entity.append(\"J\")\n",
    "                else:\n",
    "                    person_entity.append(\"C\")\n",
    "                    if len(entity[1:])>=2:\n",
    "                        for _ in range(len(entity[1:-1])):\n",
    "                            person_entity.append(\"D\")\n",
    "                        person_entity.append(\"E\")\n",
    "                    else:\n",
    "                        person_entity.append(\"F\")\n",
    "            else:\n",
    "                if len(entity_cut[0])==2:\n",
    "                    person_entity.append(\"I\")\n",
    "                context_d=True\n",
    "        else:\n",
    "            newline,_=append_entity(lin,newline)\n",
    "            newline_len=len(newline)\n",
    "            # 判断是否是连接词\n",
    "            if context_d and filter_compile.match(split_line[ind+1]) and newline_len==1:\n",
    "                person_entity.append(\"X\")\n",
    "                continue\n",
    "            # 其他非连接词\\n\",\n",
    "            if context_d and newline_len>1:\n",
    "                person_entity.append(\"B\")\n",
    "                for _ in range(len(newline[1:-1])):\n",
    "                    person_entity.append(\"Z\")\n",
    "                if filter_compile.match(split_line[ind+1]):\n",
    "                    person_entity.append(\"A\")\n",
    "                else:\n",
    "                    person_entity.append(\"Z\")\n",
    "                \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['盛世良']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline=[]\n",
    "loc_entity=[]\n",
    "org_entity=[]\n",
    "doubelsurname=['欧阳', '太史', '端木', '上官', '司马', '东方', '独孤', '南宫', '万俟', '闻人', '夏侯', '诸葛', '尉迟', '公羊', '赫连', '澹台', '皇甫', '宗政', '濮阳', '公冶', '太叔', '申屠', '公孙', '慕容', '仲孙', '钟离', '长孙', '宇文', '司徒', '鲜于', '司空', '闾丘', '子车', '亓官', '司寇', '巫马', '公西', '颛孙', '壤驷', '公良', '漆雕', '乐正', '宰父', '谷梁', '拓跋', '夹谷', '轩辕', '令狐', '段干', '百里', '呼延', '东郭', '南门', '羊舌', '微生', '公户', '公玉', '公仪', '梁丘', '公仲', '公上', '公门', '公山', '公坚', '左丘', '公伯', '西门', '公祖', '第五', '公乘', '贯丘', '公皙', '南荣', '东里', '东宫', '仲长', '子书', '子桑', '即墨', '达奚', '褚师', '吴铭']\n",
    "files='./NLP/NER/data/xinhua_dict.txt'\n",
    "f=open(files,'r')\n",
    "words=f.readlines()\n",
    "f.close()\n",
    "\n",
    "def append_entity(entity,newline):\n",
    "    entity=list(jieba.cut(entity))\n",
    "    for ent in entity:\n",
    "        newline.append(ent)\n",
    "    return newline,entity\n",
    "\n",
    "def perEntity(entity):\n",
    "    entity_tag=[]\n",
    "    entity_par=[]\n",
    "    entity_len=len(entity)\n",
    "    # 对命名实体不使用jieba分词\n",
    "    if entity_len==2:\n",
    "        if entity in words:\n",
    "            entity_tag.append(\"J-PER\")\n",
    "            entity_par.append(entity)\n",
    "        else:\n",
    "            entity_tag.extend(['C-PER','F-PER'])\n",
    "            entity_par.extend(entity)\n",
    "    else:\n",
    "        if entity[:2] in doubelsurname:\n",
    "            entity_tag.append('C-PER')\n",
    "            entity_par.append(entity[:2])\n",
    "            entity=entity[2:]\n",
    "        elif entity[:2] in words:\n",
    "            entity_tag.append(\"I-PER\")\n",
    "            entity_par.append(entity[:2])\n",
    "            entity=entity[2:]\n",
    "        else:\n",
    "            entity_tag.append(\"C-PER\")\n",
    "            entity_par.append(entity[0])\n",
    "            entity=entity[1:]\n",
    "        if len(entity)==1:\n",
    "            entity_tag.append(\"E-PER\")\n",
    "            entity_par.append(entity)\n",
    "        elif len(entity)==2:\n",
    "            if entity in words:\n",
    "                entity_tag.append(\"K-PER\")\n",
    "                entity_par.append(entity)\n",
    "            else:   \n",
    "                entity_par.extend(entity)\n",
    "                entity_tag.extend([\"D-PER\",\"E-PER\"])\n",
    "        else:\n",
    "            raise(entity,'is longer than 2')\n",
    "    return entity_tag,entity_par\n",
    "\n",
    "context_u=False #上下文的上文\n",
    "context_d=False #上下文的下文\n",
    "compile=re.compile(\"{{(.*?)}}\")\n",
    "filter_compile=re.compile(\"\\w+:\")\n",
    "split_line=compile.split(line)\n",
    "words_seq=[]\n",
    "person_entity=[]\n",
    "for ind,lin in enumerate(split_line):\n",
    "    if len(lin)==0:\n",
    "        continue\n",
    "    if filter_compile.match(lin):\n",
    "        entity_name,entity=lin.split(\":\")\n",
    "        if entity_name=='person_name':\n",
    "            entity_tag,entity_par=perEntity(entity)\n",
    "            context_d=True\n",
    "        if entity_name=='location':\n",
    "            \n",
    "            pass\n",
    "        else:\n",
    "            newline,_=append_entity(lin,newline)\n",
    "            newline_len=len(newline)\n",
    "            # 判断是否是连接词\n",
    "            if context_d and filter_compile.match(split_line[ind+1]) and newline_len==1:\n",
    "                person_entity.append(\"X\")\n",
    "                continue\n",
    "            # 其他非连接词\n",
    "            if context_d and newline_len>1:\n",
    "                person_entity.append(\"B\")\n",
    "                for _ in range(len(newline[1:-1])):\n",
    "                    person_entity.append(\"Z\")\n",
    "                if filter_compile.match(split_line[ind+1]):\n",
    "                    person_entity.append(\"A\")\n",
    "                else:\n",
    "                    person_entity.append(\"Z\")\n",
    "                \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['海淀区']\n"
     ]
    }
   ],
   "source": [
    "print(list(jieba.cut(\"海淀区\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
