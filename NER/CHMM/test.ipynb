{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n['浙江', '在线', '杭州', '4', '月', '25', '日', '讯', '（', '记者', '施宇翔', '通讯员', '方英', '）', '毒贩', '很', '“', '时髦', '”', '，', '用', '微信', '交易', '毒品', '。', '没', '料想', '警方', '也', '很', '“', '潮', '”', '，', '将计就计', '，', '一举', '将', '其', '擒获', '。', '记者', '从', '杭州', '江干区', '公安分局', '了解', '到', '，', '经过', '一个多月', '的', '侦查', '工作', '，', '江干区', '禁毒', '专案组', '抓获', '吸', '贩毒', '人员', '5', '名', '，', '缴获', '“', '冰毒', '”', '400', '余克', '，', '毒资', '30000', '余元', '，', '扣押', '汽车', '一辆', '。', '黑龙江', '籍', '男子', '钱', '某', '长期', '落脚', '于', '宾馆', '、', '单身公寓', '，', '经常', '变换', '住址', '。', '他', '有', '一辆车', '，', '经常', '半夜', '驾车', '来往', '于', '杭州', '主城区', '的', '各大', '宾馆', '和', '单身公寓', '，', '并且', '常要', '活动', '到', '凌晨', '6', '、', '7', '点钟', '，', '白天', '则', '在', '家里', '呼呼大睡', '。', '钱', '某', '不', '寻常', '的', '特征', '，', '引起', '了', '警方', '注意', '。', '禁毒', '大队', '通过', '侦查', '，', '发现', '钱', '某', '实际上', '是', '在', '向', '落脚', '于', '宾馆', '和', '单身公寓', '的', '吸毒', '人员', '贩送', '“', '冰毒', '”', '。'] ['B-PRO', 'I-PRO', 'I-PRO', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "files='/Users/lj/PycharmProjects/NLP/NER/data/BosonNLP_NER_6C.txt'\n",
    "a={'I-TIME', 'I-PER', 'B-CRIME', 'I-CRIME', 'B-TIME', 'B-LOC', 'B-ORG', \n",
    "   'B-ROLE', 'B-PER', 'I-ROLE', 'I-ORG', 'I-LOC', 'I-LAW', 'B-LAW', 'O'}\n",
    "import jieba,re\n",
    "f=open(files,'r')\n",
    "lines=f.readlines()\n",
    "compiles=re.compile(\"{{(.*?)}}\")\n",
    "filter_compile=re.compile(\"[a-zA-Z_]+:\")\n",
    "tags=[]\n",
    "words=[]\n",
    "for line in lines:\n",
    "    split_line=compiles.split(line)\n",
    "    tag=[]\n",
    "    word=[]\n",
    "    for lin in split_line:\n",
    "        if len(lin.strip())==0:\n",
    "            continue\n",
    "        if filter_compile.match(lin):\n",
    "            entity_name,entity=lin.split(\":\",1)\n",
    "            entity=list(jieba.cut(entity))\n",
    "            entities=[a.strip() for a in entity if len(a.strip())!=0]\n",
    "            word.extend(entities)\n",
    "            if entity_name=='person_name':\n",
    "                tag.append(\"B-PER\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-PER\"]*(len(entities)-1))\n",
    "            elif entity_name=='time':\n",
    "                tag.append(\"B-TIME\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-TIME\"]*(len(entities)-1))\n",
    "            elif entity_name=='location':\n",
    "                tag.append(\"B-LOC\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-LOC\"]*(len(entities)-1))\n",
    "            elif entity_name=='org_name':\n",
    "                tag.append(\"B-ORG\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-ORG\"]*(len(entities)-1))\n",
    "            elif entity_name=='company_name':\n",
    "                tag.append(\"B-COMP\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-COMP\"]*(len(entities)-1))\n",
    "            elif entity_name=='product_name':\n",
    "                tag.append(\"B-PRO\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-PRO\"]*(len(entities)-1))\n",
    "        else:\n",
    "            entity=list(jieba.cut(lin))\n",
    "            entities=[a.strip() for a in entity if len(a.strip())!=0]\n",
    "            word.extend(entities)\n",
    "            tag.extend([\"O\"]*len(entities))\n",
    "    if len(word)!=len(tag):\n",
    "        raise ValueError(len(word),len(tag),line)\n",
    "    words.append(word)\n",
    "    tags.append(tag)\n",
    "print(len(words),len(tags))\n",
    "files1='/Users/lj/PycharmProjects/NLP/NER/data/target.txt'\n",
    "files2='/Users/lj/PycharmProjects/NLP/NER/data/source.txt'\n",
    "f1=open(files1,'w')\n",
    "for tag in tags:\n",
    "    f1.write(\" \".join(tag)+\"\\n\")\n",
    "f1.close()\n",
    "f2=open(files2,'w')\n",
    "for word in words:\n",
    "    f2.write(\" \".join(word)+\"\\n\")\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity=[\"time\",\"location\",\"person_name\",\"org_name\",\"company_name\",\"product_name\"]\n",
    "import re,jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雷格・罗\n罗恩·康维\n凯文·罗斯\n阿什顿·库彻\n西罗•库泽\n西罗•库泽\n卡洛斯·吉利纳尔"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "吉利纳尔",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c16fbbb8b050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mentity_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mentity_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'person_name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mentity_tag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity_par\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperEntity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_par\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-c16fbbb8b050>\u001b[0m in \u001b[0;36mperEntity\u001b[0;34m(entity)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mentity_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"D-PER\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"E-PER\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentity_tag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity_par\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 吉利纳尔"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "line=\"{{product_name:浙江在线杭州}}{{time:4月25日}}讯（记者{{person_name: 施宇翔}} 通讯员 {{person_name:方英}}）毒贩很“时髦”，用{{product_name:微信}}交易毒品。没料想警方也很“潮”，将计就计，一举将其擒获。记者从{{org_name:杭州江干区公安分局}}了解到，经过一个多月的侦查工作，{{org_name:江干区禁毒专案组}}抓获吸贩毒人员5名，缴获“冰毒”400余克，毒资30000余元，扣押汽车一辆。{{location:黑龙江}}籍男子{{person_name:钱某}}长期落脚于宾馆、单身公寓，经常变换住址。他有一辆车，经常半夜驾车来往于{{location:杭州主城区}}的各大宾馆和单身公寓，并且常要活动到{{time:凌晨6、7点钟}}，{{time:白天}}则在家里呼呼大睡。{{person_name:钱某}}不寻常的特征，引起了警方注意。禁毒大队通过侦查，发现{{person_name:钱某}}实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。\"\n",
    "newline=[]\n",
    "loc_entity=[]\n",
    "org_entity=[]\n",
    "doubelsurname=['欧阳', '太史', '端木', '上官', '司马', '东方', '独孤', '南宫', '万俟', '闻人', '夏侯', '诸葛', '尉迟', '公羊', '赫连', '澹台', '皇甫', '宗政', '濮阳', '公冶', '太叔', '申屠', '公孙', '慕容', '仲孙', '钟离', '长孙', '宇文', '司徒', '鲜于', '司空', '闾丘', '子车', '亓官', '司寇', '巫马', '公西', '颛孙', '壤驷', '公良', '漆雕', '乐正', '宰父', '谷梁', '拓跋', '夹谷', '轩辕', '令狐', '段干', '百里', '呼延', '东郭', '南门', '羊舌', '微生', '公户', '公玉', '公仪', '梁丘', '公仲', '公上', '公门', '公山', '公坚', '左丘', '公伯', '西门', '公祖', '第五', '公乘', '贯丘', '公皙', '南荣', '东里', '东宫', '仲长', '子书', '子桑', '即墨', '达奚', '褚师', '吴铭']\n",
    "files='./NER/data/xinhua_dict.txt'\n",
    "f=open(files,'r')\n",
    "wordsdict=f.readlines()\n",
    "f.close()\n",
    "ran_compile=re.compile(\"[^a-zA-Z\\u4e00-\\u9fa5]\")\n",
    "en_name_compile=re.compile('[^\\u4e00-\\u9fa5]')\n",
    "def perEntity(entity):\n",
    "    entity_tag=[]\n",
    "    entity_par=[]\n",
    "    if en_name_compile.match(entity):\n",
    "        entities=entity.split()\n",
    "        entity_tag.append(\"C-PER\")\n",
    "        entity_par.extend(entities)\n",
    "        if len(entities)!=1:\n",
    "            for _ in range(len(entities[1:-1])):\n",
    "                entity_tag.append(\"D-PER\")\n",
    "            entity_tag.append(\"E-PER\")\n",
    "        return entity_tag,entity_par\n",
    "    if '・' in entity or '·' in entity or '•' in entity:\n",
    "        print(entity)\n",
    "        entities=entity.split('·|・|•')\n",
    "        entity_tag.append(\"C-PER\")\n",
    "        entity_par.extend(entities)\n",
    "        if len(entities)!=1:\n",
    "            for _ in range(len(entities[1:-1])):\n",
    "                entity_tag.append(\"D-PER\")\n",
    "            entity_tag.append(\"E-PER\")\n",
    "        return entity_tag,entity_par\n",
    "    \n",
    "    entity_len=len(entity)\n",
    "    # 对命名实体不使用jieba分词\n",
    "    if entity_len==2:\n",
    "        if entity in wordsdict:\n",
    "            entity_tag.append(\"J-PER\")\n",
    "            entity_par.append(entity)\n",
    "        else:\n",
    "            entity_tag.extend(['C-PER','F-PER'])\n",
    "            entity_par.extend(entity)\n",
    "    else:\n",
    "        if entity[:2] in doubelsurname:\n",
    "            entity_tag.append('C-PER')\n",
    "            entity_par.append(entity[:2])\n",
    "            entities=entity[2:]\n",
    "        elif entity[:2] in wordsdict:\n",
    "            entity_tag.append(\"I-PER\")\n",
    "            entity_par.append(entity[:2])\n",
    "            entities=entity[2:]\n",
    "        else:\n",
    "            entity_tag.append(\"C-PER\")\n",
    "            entity_par.append(entity[0])\n",
    "            entities=entity[1:]\n",
    "            \n",
    "        if len(entities)==1:\n",
    "            entity_tag.append(\"E-PER\")\n",
    "            entity_par.append(entities)\n",
    "        elif len(entities)==2:\n",
    "            if entities in wordsdict:\n",
    "                entity_tag.append(\"K-PER\")\n",
    "                entity_par.append(entities)\n",
    "            else:   \n",
    "                entity_par.extend(entities)\n",
    "                entity_tag.extend([\"D-PER\",\"E-PER\"])\n",
    "        else:\n",
    "            raise ValueError(entity)\n",
    "    return entity_tag,entity_par\n",
    "\n",
    "context_u=False #上下文的上文\n",
    "context_d=False #上下文的下文\n",
    "compile=re.compile(\"{{(.*?)}}\")\n",
    "filter_compile=re.compile(\"[a-zA-Z_]+:\")\n",
    "files='/Users/lj/PycharmProjects/NLP/NER/data/BosonNLP_NER_6C.txt'\n",
    "f=open(files,'r')\n",
    "lines=f.readlines()\n",
    "words=[]\n",
    "tags=[]\n",
    "for line in lines:\n",
    "    split_line=compile.split(line)\n",
    "    word=[]\n",
    "    tag=[]\n",
    "    for lin in split_line:\n",
    "        if len(lin.strip())==0:\n",
    "            continue\n",
    "        if filter_compile.match(lin):\n",
    "            entity_name,entity=lin.split(\":\",1)\n",
    "            if entity_name=='person_name':\n",
    "                entity_tag,entity_par=perEntity(entity.strip())\n",
    "                tag.extend(entity_tag)\n",
    "                word.extend(entity_par)\n",
    "            else:\n",
    "                entity_par=list(jieba.cut(entity.strip()))\n",
    "                word.extend(entity_par)\n",
    "                tag.extend(['Z-PER']*len(entity_par))      \n",
    "        else:\n",
    "            entity_par=list(jieba.cut(lin.strip()))\n",
    "            word.extend(entity_par)\n",
    "            tag.extend(['Z-PER']*len(entity_par))\n",
    "    \n",
    "    if len(word)!=len(tag):\n",
    "        raise ValueError(len(word),len(tag),line)\n",
    "    words.append(word)\n",
    "    tags.append(tag)\n",
    "\n",
    "print(len(words),len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雷格罗\n"
     ]
    }
   ],
   "source": [
    "a='雷格・罗'\n",
    "ran_compile=re.compile(\"[^a-zA-Z\\u4e00-\\u9fa5]\")\n",
    "print(ran_compile.sub('',a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
