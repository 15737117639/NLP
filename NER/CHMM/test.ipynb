{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files='/Users/lj/PycharmProjects/NLP/NER/data/BosonNLP_NER_6C.txt'\n",
    "a={'I-TIME', 'I-PER', 'B-CRIME', 'I-CRIME', 'B-TIME', 'B-LOC', 'B-ORG', \n",
    "   'B-ROLE', 'B-PER', 'I-ROLE', 'I-ORG', 'I-LOC', 'I-LAW', 'B-LAW', 'O'}\n",
    "import jieba,re\n",
    "f=open(files,'r')\n",
    "lines=f.readlines()\n",
    "compiles=re.compile(\"{{(.*?)}}\")\n",
    "filter_compile=re.compile(\"[a-zA-Z_]+:\")\n",
    "tags=[]\n",
    "words=[]\n",
    "for line in lines:\n",
    "    split_line=compiles.split(line)\n",
    "    tag=[]\n",
    "    word=[]\n",
    "    for lin in split_line:\n",
    "        if len(lin.strip())==0:\n",
    "            continue\n",
    "        if filter_compile.match(lin):\n",
    "            entity_name,entity=lin.split(\":\",1)\n",
    "            entity=list(jieba.cut(entity))\n",
    "            entities=[a.strip() for a in entity if len(a.strip())!=0]\n",
    "            word.extend(entities)\n",
    "            if entity_name=='person_name':\n",
    "                tag.append(\"B-PER\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-PER\"]*(len(entities)-1))\n",
    "            elif entity_name=='time':\n",
    "                tag.append(\"B-TIME\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-TIME\"]*(len(entities)-1))\n",
    "            elif entity_name=='location':\n",
    "                tag.append(\"B-LOC\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-LOC\"]*(len(entities)-1))\n",
    "            elif entity_name=='org_name':\n",
    "                tag.append(\"B-ORG\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-ORG\"]*(len(entities)-1))\n",
    "            elif entity_name=='company_name':\n",
    "                tag.append(\"B-COMP\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-COMP\"]*(len(entities)-1))\n",
    "            elif entity_name=='product_name':\n",
    "                tag.append(\"B-PRO\")\n",
    "                if len(entity)!=0:\n",
    "                    tag.extend([\"I-PRO\"]*(len(entities)-1))\n",
    "        else:\n",
    "            entity=list(jieba.cut(lin))\n",
    "            entities=[a.strip() for a in entity if len(a.strip())!=0]\n",
    "            word.extend(entities)\n",
    "            tag.extend([\"O\"]*len(entities))\n",
    "    if len(word)!=len(tag):\n",
    "        raise ValueError(len(word),len(tag),line)\n",
    "    words.append(word)\n",
    "    tags.append(tag)\n",
    "print(len(words),len(tags))\n",
    "files1='/Users/lj/PycharmProjects/NLP/NER/data/target.txt'\n",
    "files2='/Users/lj/PycharmProjects/NLP/NER/data/source.txt'\n",
    "f1=open(files1,'w')\n",
    "for tag in tags:\n",
    "    f1.write(\" \".join(tag)+\"\\n\")\n",
    "f1.close()\n",
    "f2=open(files2,'w')\n",
    "for word in words:\n",
    "    f2.write(\" \".join(word)+\"\\n\")\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity=[\"time\",\"location\",\"person_name\",\"org_name\",\"company_name\",\"product_name\"]\n",
    "import re,jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "('安切洛蒂', '切洛蒂')",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0fa1be31b3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mentity_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mentity_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'person_name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mentity_tag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity_par\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperEntity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_par\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-0fa1be31b3da>\u001b[0m in \u001b[0;36mperEntity\u001b[0;34m(entity)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msin_entity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msin_entity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msin_entity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'D-PER'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msin_entity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ('安切洛蒂', '切洛蒂')"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "line=\"{{product_name:浙江在线杭州}}{{time:4月25日}}讯（记者{{person_name: 施宇翔}} 通讯员 {{person_name:方英}}）毒贩很“时髦”，用{{product_name:微信}}交易毒品。没料想警方也很“潮”，将计就计，一举将其擒获。记者从{{org_name:杭州江干区公安分局}}了解到，经过一个多月的侦查工作，{{org_name:江干区禁毒专案组}}抓获吸贩毒人员5名，缴获“冰毒”400余克，毒资30000余元，扣押汽车一辆。{{location:黑龙江}}籍男子{{person_name:钱某}}长期落脚于宾馆、单身公寓，经常变换住址。他有一辆车，经常半夜驾车来往于{{location:杭州主城区}}的各大宾馆和单身公寓，并且常要活动到{{time:凌晨6、7点钟}}，{{time:白天}}则在家里呼呼大睡。{{person_name:钱某}}不寻常的特征，引起了警方注意。禁毒大队通过侦查，发现{{person_name:钱某}}实际上是在向落脚于宾馆和单身公寓的吸毒人员贩送“冰毒”。\"\n",
    "newline=[]\n",
    "loc_entity=[]\n",
    "org_entity=[]\n",
    "doubelsurname=['欧阳', '太史', '端木', '上官', '司马', '东方', '独孤', '南宫', '万俟', '闻人', '夏侯', '诸葛', '尉迟', '公羊', '赫连', '澹台', '皇甫', '宗政', '濮阳', '公冶', '太叔', '申屠', '公孙', '慕容', '仲孙', '钟离', '长孙', '宇文', '司徒', '鲜于', '司空', '闾丘', '子车', '亓官', '司寇', '巫马', '公西', '颛孙', '壤驷', '公良', '漆雕', '乐正', '宰父', '谷梁', '拓跋', '夹谷', '轩辕', '令狐', '段干', '百里', '呼延', '东郭', '南门', '羊舌', '微生', '公户', '公玉', '公仪', '梁丘', '公仲', '公上', '公门', '公山', '公坚', '左丘', '公伯', '西门', '公祖', '第五', '公乘', '贯丘', '公皙', '南荣', '东里', '东宫', '仲长', '子书', '子桑', '即墨', '达奚', '褚师', '吴铭']\n",
    "files='./NLP/NER/data/xinhua_dict.txt'\n",
    "f=open(files,'r')\n",
    "wordsdict=f.readlines()\n",
    "f.close()\n",
    "ran_compile=re.compile(\"[^a-zA-Z\\u4e00-\\u9fa5]\")\n",
    "en_name_compile=re.compile('[^\\u4e00-\\u9fa5]')\n",
    "def perEntity(entity):        \n",
    "    tag=[]\n",
    "    word=[]\n",
    "    if en_name_compile.match(entity):\n",
    "        entities=entity.split()\n",
    "        word.extend(entities)\n",
    "        tag.append(\"C-PER\")\n",
    "        for _ in range(len(entities)-2):\n",
    "            tag.append('D-PER')\n",
    "        if len(entities)>=2:\n",
    "            tag.append(\"E-PER\")\n",
    "        return tag,word\n",
    "    entities=list(jieba.cut(entity.strip(),HMM=False))\n",
    "    entity_len=len(entities)\n",
    "    # 对命名实体不使用jieba分词\n",
    "    if entity_len==1:\n",
    "        sin_entity=entity\n",
    "        if len(sin_entity)==1:\n",
    "            tag.append(\"C-PER\")\n",
    "            word.append(sin_entity)\n",
    "        elif len(sin_entity)==2:\n",
    "            # 通过jieba分词，如果人名的命名实体只有两个词，并且jieba把其分成一个词，那么就认为其是一个词组。\n",
    "            tag.append(\"J-PER\")\n",
    "            word.append(sin_entity)\n",
    "        else:\n",
    "            if sin_entity[:2] in doubelsurname:\n",
    "                tag.append('C-PER')\n",
    "                word.append(entity[:2])\n",
    "                sin_entity=sin_entity[2:]\n",
    "            # jieba可能会将三个字的人名分成一个词组，但是其\n",
    "            elif sin_entity[:2] in wordsdict:\n",
    "                tag.append(\"I-PER\")\n",
    "                word.append(sin_entity[:2])\n",
    "                sin_entity=sin_entity[2:]\n",
    "            else:\n",
    "                tag.append('C-PER')\n",
    "                word.append(sin_entity[0])\n",
    "                sin_entity=sin_entity[1:]\n",
    "                \n",
    "            if len(sin_entity)==1:\n",
    "                tag.append('E-PER')\n",
    "                word.append(sin_entity)\n",
    "            elif len(sin_entity)==2 and (sin_entity in wordsdict):\n",
    "                tag.append('K-PER')\n",
    "                word.append(sin_entity)\n",
    "            else:\n",
    "                assert len(sin_entity)==2,(entity,sin_entity)\n",
    "                tag.append('D-PER')\n",
    "                word.extend(sin_entity)\n",
    "                tag.append('E-PER')\n",
    "    else:\n",
    "        word.extend(entities)\n",
    "        if len(entities[0])==2:\n",
    "            if entities[0] in doubelsurname:\n",
    "                tag.append('C-PER')\n",
    "            else:\n",
    "                tag.append('J-PER')\n",
    "            \n",
    "        else:\n",
    "            tag.append(\"C-PER\")\n",
    "        entities=entities[1:]\n",
    "        if len(entities)==1 and len(entities[0])==2:\n",
    "            tag.append(\"K-PER\")\n",
    "        elif len(entities)==1 and len(entities[0])==1:\n",
    "            tag.append(\"F-PER\")\n",
    "        else:\n",
    "            for _ in range(len(entities)-1):\n",
    "                tag.append(\"D-PER\")\n",
    "            tag.append(\"E-PER\")\n",
    "    return tag,word\n",
    "\n",
    "context_u=False #上下文的上文\n",
    "context_d=False #上下文的下文\n",
    "compile=re.compile(\"{{(.*?)}}\")\n",
    "filter_compile=re.compile(\"[a-zA-Z_]+:\")\n",
    "files='./NLP/NER/data/BosonNLP_NER_6C.txt'\n",
    "f=open(files,'r')\n",
    "lines=f.readlines()\n",
    "words=[]\n",
    "tags=[]\n",
    "for line in lines:\n",
    "    split_line=compile.split(line)\n",
    "    word=[]\n",
    "    tag=[]\n",
    "    for lin in split_line:\n",
    "        if len(lin.strip())==0:\n",
    "            continue\n",
    "        if filter_compile.match(lin):\n",
    "            entity_name,entity=lin.split(\":\",1)\n",
    "            if entity_name=='person_name':\n",
    "                entity_tag,entity_par=perEntity(entity)\n",
    "                tag.extend(entity_tag)\n",
    "                word.extend(entity_par)\n",
    "            else:\n",
    "                entities=list(jieba.cut(entity.strip(),HMM=False))\n",
    "                word.extend(entities)\n",
    "                tag.extend(['Z-PER']*len(entities))      \n",
    "        else:\n",
    "            entity_par=list(jieba.cut(lin.strip()))\n",
    "            word.extend(entity_par)\n",
    "            tag.extend(['Z-PER']*len(entity_par))\n",
    "    \n",
    "    if len(word)!=len(tag):\n",
    "        raise ValueError(len(word),len(tag),line)\n",
    "    words.append(word)\n",
    "    tags.append(tag)\n",
    "\n",
    "print(len(words),len(tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2012', '年', '1', '月', '8', '日', '，', 'START', '安切洛蒂', 'END', '入主', '巴黎', '圣日耳曼', '的', '首场', '比赛', '，', '便是', '在', '法国', '杯', '1', '/', '32', '决赛', '中', '对阵', '低', '级别', '球队', '洛克', '米', '内', '，', '是', '役', '凭借', 'START', '卢', '加', '诺', 'END', '在', '伤停补时', '阶段', '的', '进球', '，', '首都', '球队', '方才', '艰难', '取胜', '。', '正是', '由于', '拥有', '这样', '的', '经历', '，', 'START', '安切洛蒂', 'END', '才', '不会', '轻视', '阿拉斯', '，', '他', '说道', '：', '“', '法国', '杯', '是', '一个', '重要', '的', '目标', '，', '每个', '人', '都', '知道', '这项', '赛事', '的', '重要性', '。', '这', '是', '一项', '美妙', '的', '赛事', '，', '所有人', '都', '希望', '夺得', '冠军', '。', '上赛季', '我', '执教', '球队', '的', '首场', '比赛', '，', '球队', '击败', '洛克', '米', '内', '的', '过程', '便', '异常', '艰难', '，', '周日', '的', '比赛', '也', '不会', '太', '轻松', '。', '”', '当', '被', '问及', '曼城', '前锋', 'START', '巴洛特', '利', 'END', '加盟', '巴黎', '圣日耳曼', '的', '可能性', '时', '，', 'START', '安切洛蒂', 'END', '斩钉截铁', '的', '回答', '道', '：', '“', '我', '不', '认为', '球队', '需要', 'START', '巴洛特', '利', 'END', '。', '他', '是', '一名', '非常', '出色', '的', '球员', '，', '不过', '我们', '在', '前锋线', '上', '拥有', 'START', '伊布拉', 'END', '，', '因此', '不', '需要', 'START', '巴洛特', '利', 'END', '。', '目前', '球队', '的', '攻击力', '极具', '竞争力', '。', '”']\n9\n41\n63\n139\n149\n164\n182\n190\n[7, 9, 37, 41, 61, 63, 136, 139, 147, 149, 161, 164, 180, 182, 187, 190]\n"
     ]
    }
   ],
   "source": [
    "line=\"{{time:2012年1月8日}}，{{person_name:安切洛蒂}}入主{{org_name:巴黎圣日耳曼}}的首场比赛，便是在{{location:法国}}杯1/32决赛中对阵低级别球队{{org_name:洛克米内}}，是役凭借{{person_name:卢加诺}}在伤停补时阶段的进球，{{org_name:首都球队}}方才艰难取胜。正是由于拥有这样的经历，{{person_name:安切洛蒂}}才不会轻视{{org_name:阿拉斯}}，他说道：“{{location:法国}}杯是一个重要的目标，每个人都知道这项赛事的重要性。这是一项美妙的赛事，所有人都希望夺得冠军。{{time:上赛季}}我执教球队的首场比赛，球队击败{{org_name:洛克米内}}的过程便异常艰难，{{time:周日}}的比赛也不会太轻松。”当被问及{{org_name:曼城}}前锋{{person_name:巴洛特利}}加盟{{org_name:巴黎圣日耳曼}}的可能性时，{{person_name:安切洛蒂}}斩钉截铁的回答道：“我不认为球队需要{{person_name:巴洛特利}}。他是一名非常出色的球员，不过我们在前锋线上拥有{{person_name:伊布拉}}，因此不需要{{person_name:巴洛特利}}。目前球队的攻击力极具竞争力。”\"\n",
    "\n",
    "entity_tag={\"TIME\":\"time\",\"LOC\":\"location\",\"PER\":\"person_name\",\"ORG\":\"org_name\",\"COMP\":\"company_name\",\"PRO\":\"product_name\"}\n",
    "import jieba,re\n",
    "def drop_tag(seqence,tag):\n",
    "    compiles=re.compile(\"{{(.*?)}}\")\n",
    "    filter_compile=re.compile(\"[a-zA-Z_]+:\")\n",
    "    seqences=compiles.split(seqence)\n",
    "    seqs=[]\n",
    "    for seq in seqences:\n",
    "        if len(seq.strip())==0:\n",
    "            continue\n",
    "        if filter_compile.match(seq):\n",
    "            name,entity=seq.split(\":\",1)\n",
    "            if name==entity_tag[tag]:\n",
    "                seqs.append(\"START\"+entity+\"END\")\n",
    "            else:\n",
    "                seqs.append(entity)\n",
    "        else:\n",
    "            seqs.append(seq)\n",
    "    return \"\".join(seqs)\n",
    "\n",
    "def cut_seq(seqence):\n",
    "    cuts=list(jieba.cut(seqence,HMM=False))\n",
    "    cut=[a.strip() for a in cuts if len(a.strip())!=0]\n",
    "    return cut\n",
    "\n",
    "def find_tag(seqs,tag=\"START\"):\n",
    "    if isinstance(tag,list):\n",
    "        index=[ind for ind,word in enumerate(seqs) if word in tag]\n",
    "    else:\n",
    "        index=[ind for ind,word in enumerate(seqs) if word==tag]\n",
    "    return index\n",
    "\n",
    "seqences=drop_tag(line,\"PER\")\n",
    "seqs=cut_seq(seqences)\n",
    "init_seq=re.sub(\"START|END\",'',seqences)\n",
    "init_seqs=cut_seq(init_seq)\n",
    "\n",
    "new_seq=[]\n",
    "tag=[]\n",
    "print(seqs)\n",
    "index=find_tag(seqs,tag=[\"START\",\"END\"])\n",
    "index=sorted(index)\n",
    "last=0\n",
    "for ind in range(1,len(index),2):\n",
    "    start=index[ind-1]\n",
    "    end=index[ind]\n",
    "    tag.append(['Z-PER']*(start-last-2))\n",
    "    tag.append(\"A-PER\")\n",
    "    xrange=end-start-1\n",
    "    if xrange==1:\n",
    "        tag.append('J-PER')\n",
    "    elif xrange==2:\n",
    "        if len(seqs[0])>=2:\n",
    "            if seqs[0] in doubelsurname:\n",
    "                tag.append(\"C-PER\")\n",
    "            else:\n",
    "                tag.append(\"I-PER\")\n",
    "                \n",
    "            if len(seqs[1])==2:\n",
    "                tag.append(\"K-PER\")\n",
    "            else:\n",
    "                tag.append(\"E-PER\")\n",
    "        else:\n",
    "            tag.append(\"C-PER\")\n",
    "            if len(seqs[1])>=2:\n",
    "                tag.append(\"K-PER\")\n",
    "            else:\n",
    "                tag.append(\"F-PER\")\n",
    "\n",
    "    else:\n",
    "        if len(seqs[0])>=2:\n",
    "            if seqs[0] in doubelsurname:\n",
    "                tag.append(\"C-PER\")\n",
    "            else:\n",
    "                tag.append(\"I-PER\")\n",
    "        else:\n",
    "            tag.append(\"C-PER\")\n",
    "        \n",
    "            \n",
    "    last=end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 9, 21, 37, 41, 61, 63, 136, 139, 147, 149, 161, 164, 180, 182, 187, 190, 234]\n"
     ]
    }
   ],
   "source": [
    "a=[21,234,7, 9, 37, 41, 61, 63, 136, 139, 147, 149, 161, 164, 180, 182, 187, 190]\n",
    "a=sorted(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['安切洛蒂']\n"
     ]
    }
   ],
   "source": [
    "print(list(jieba.cut(\"安切洛蒂\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n2\n4\n6\n8\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10,2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
